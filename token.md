æ€»ä½“ç­–ç•¥ï¼šæ„å»ºå‰å…ˆé¢„ä¼° token æ•°ï¼ŒæŒ‰éœ€åˆ‡åˆ†æäº¤
âœ… ä¸€ã€token é™åˆ¶æ„æˆè¯´æ˜
ä»¥ Grok 3 Mini ä¸ºä¾‹ï¼š

æœ€å¤§ token æ€»æ•°ï¼š131072 tokens

åŒ…æ‹¬ï¼š

âœ… systemPromptï¼ˆç³»ç»Ÿæç¤ºè¯ï¼‰

âœ… userPromptï¼ˆä½ æ‹¼æ¥å¥½çš„ [SEG#]\n... å†…å®¹ï¼‰

âœ… AI å›å¤ï¼ˆå³è¯‘æ–‡è¾“å‡ºï¼‰

ğŸ§  æ¨èå®‰å…¨é™é¢ï¼ˆä¸º AI å›å¤é¢„ç•™ç©ºé—´ï¼‰ï¼š

æ¨¡å‹	æœ€å¤§ tokens	æ¨è maxInputTokensï¼ˆsystem+userï¼‰	ç•™ç»™è¯‘æ–‡
Grok 3 Mini	131072	âœ… å»ºè®®æ§åˆ¶åœ¨ 40000 tokens å†…	~30,000 tokens
GPT-4o	128,000	âœ… å»ºè®®æ§åˆ¶åœ¨ 96,000 tokens å†…	~32,000 tokens
âœ… äºŒã€è®¡ç®— token æ€»æ•°ï¼ˆæ„å»ºé˜¶æ®µï¼‰
ä½ å¯ä»¥ä½¿ç”¨ tiktoken æ¥è¯„ä¼°ï¼š

ts
å¤åˆ¶
ç¼–è¾‘
import { encoding_for_model } from "tiktoken";

const enc = encoding_for_model("gpt-4"); // Grok ç›®å‰å…¼å®¹ GPT tokenizer
const tokenCount = (text) => enc.encode(text).length;

const totalInputTokens = tokenCount(systemPrompt) + tokenCount(userPrompt);
if (totalInputTokens > 100000) {
  // åˆ‡æ®µå¤„ç†
}
âœ… ä¸‰ã€è‡ªåŠ¨æŒ‰ token æ‹†åˆ†ï¼ˆè€Œéå›ºå®šæ®µæ•°ï¼‰
ä½ å¯ä»¥è¿™æ ·åŠ¨æ€æ‹†æ‰¹ï¼š

ts
å¤åˆ¶
ç¼–è¾‘
function splitSegmentsByTokenLimit(segments, systemPrompt, maxTokens = 100000) {
  const enc = encoding_for_model("gpt-4");
  const systemTokens = enc.encode(systemPrompt).length;

  let batches = [];
  let currentBatch = [];
  let currentTokens = systemTokens;

  for (let seg of segments) {
    const thisSegText = `[SEG${seg.index}]\n${seg.sourceText}`;
    const thisTokens = enc.encode(thisSegText).length;

    if (currentTokens + thisTokens > maxTokens) {
      batches.push(currentBatch);
      currentBatch = [seg];
      currentTokens = systemTokens + thisTokens;
    } else {
      currentBatch.push(seg);
      currentTokens += thisTokens;
    }
  }

  if (currentBatch.length) {
    batches.push(currentBatch);
  }

  return batches;
}
âœ… å››ã€æ¯ä¸€æ‰¹åˆ†åˆ«ç¿»è¯‘ + å›å¡«
ä½ æœ€ç»ˆå¾—åˆ°ï¼š

ts
å¤åˆ¶
ç¼–è¾‘
const batches = splitSegmentsByTokenLimit(segments, systemPrompt);
for (const batch of batches) {
  const userPrompt = buildSegmentedPrompt(batch);
  const response = await translate(systemPrompt, userPrompt);
  const resultMap = parseTranslatedSegments(response);
  // å›å¡«æ•°æ®åº“
}
âœ… æ€»ç»“ï¼šæ„å»ºæ—¶å¦‚ä½•æ§åˆ¶ token é™åˆ¶ï¼Ÿ

æ­¥éª¤	æ“ä½œ
âœ… è·å–ç³»ç»Ÿæç¤º token æ•°	tokenCount(systemPrompt)
âœ… æ‹¼æ¥ç”¨æˆ·æ®µè½å¹¶ç´¯åŠ  token	tokenCount([SEG#]\n...)
âœ… æ§åˆ¶æ€» token ä¸è¶…è®¾å®šä¸Šé™	æ¨è â‰¤ 100,000
âœ… è‡ªåŠ¨æ‹†æ‰¹å¤„ç†è¶…é•¿å†…å®¹	ä¿è¯æ¯æ‰¹æäº¤éƒ½ä¸è¶…é™
âœ… ä¸ºè¾“å‡ºé¢„ç•™ç©ºé—´	30,000 ~ 40,000 tokens å®‰å…¨